{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hmf43eOzQQUU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.cuda.amp as amp\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "epochs = 5\n",
    "batch_size = 1024 \n",
    "lr = 3e-4\n",
    "weight_decay = 0.01\n",
    "device = \"cuda\"\n",
    "checkpoint_filepath = None  # Set to a path if you want to load a checkpoint\n",
    "save_dir = \"checkpoints\"\n",
    "import os\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zfdeGJwXQIHU"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that converts image patches to embeddings for Vision Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 image_size: tuple = (64, 72),\n",
    "                 patch_size: int = 8,\n",
    "                 in_channels: int = 3,\n",
    "                 embedding_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size)\n",
    "\n",
    "        # Create projection for converting patches to embeddings\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        # CLS token embedding\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "\n",
    "        # Positional embedding (Normal distribution initialization of value)\n",
    "        self.positions = nn.Parameter(torch.zeros(1, self.num_patches + 1, embedding_dim))\n",
    "        nn.init.trunc_normal_(self.positions, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Convert image to patches and project to embedding dimension\n",
    "        # x shape: [batch_size, channels, height, width]\n",
    "        x = self.projection(x)\n",
    "        # x shape: [batch_size, embedding_dim, height/patch_size, width/patch_size]\n",
    "\n",
    "        # Flatten patches to sequence\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        # x shape: [batch_size, num_patches, embedding_dim]\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.positions\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
    "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
    "        self.scaler = 1/math.sqrt(head_dim)\n",
    "        self.lora_qkv_a = nn.Linear(hidden_dim, lora_rank)\n",
    "        self.lora_qkv_b = nn.Linear(lora_rank, (q_head+kv_head*2)*head_dim)\n",
    "        self.lora_o_a = nn.Linear(q_head*head_dim, lora_rank)\n",
    "        self.lora_o_b = nn.Linear(lora_rank, hidden_dim)\n",
    "\n",
    "        if q_head != kv_head:\n",
    "            # If we are using multi query attention\n",
    "            assert q_head % kv_head == 0\n",
    "            self.multi_query_attention = True\n",
    "            self.q_kv_scale = q_head//kv_head\n",
    "        else:\n",
    "            self.multi_query_attention = False\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len, hid_dim = tensor.shape\n",
    "\n",
    "        qkv_tensor = self.qkv(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_qkv_a(tensor)\n",
    "            lora_tensor = self.lora_qkv_b(lora_tensor)\n",
    "            qkv_tensor = lora_tensor + qkv_tensor\n",
    "        query, key, value = qkv_tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
    "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            # If we are using multi query attention, duplicate key value heads\n",
    "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
    "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
    "\n",
    "        # Switch to batch_size, head, seq_len, head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Classic self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        if attention_mask != None:\n",
    "            attention_scaled += attention_mask\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, seq_len, hid_dim\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.view(batch_size, seq_len, hid_dim)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_o_a(value)\n",
    "            lora_tensor = self.lora_o_b(lora_tensor)\n",
    "            output = lora_tensor + output\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.gate_and_up = nn.Linear(hidden_size, hidden_size * expansion_factor * 2)\n",
    "        self.down = nn.Linear(hidden_size * expansion_factor, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        self.lora_gate_and_up_a = nn.Linear(hidden_size, lora_rank)\n",
    "        self.lora_gate_and_up_b = nn.Linear(lora_rank, hidden_size * expansion_factor * 2)\n",
    "        self.lora_down_a = nn.Linear(hidden_size * expansion_factor, lora_rank)\n",
    "        self.lora_down_b = nn.Linear(lora_rank, hidden_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        gate_and_up = self.gate_and_up(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_gate_and_up_a(tensor)\n",
    "            lora_tensor = self.lora_gate_and_up_b(lora_tensor)\n",
    "            gate_and_up = gate_and_up + lora_tensor\n",
    "        gate, up = gate_and_up.chunk(chunks=2, dim=-1)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        tensor = gate * up\n",
    "        tensor = self.dropout(tensor)\n",
    "        down_tensor = self.down(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_down_a(tensor)\n",
    "            lora_tensor = self.lora_down_b(lora_tensor)\n",
    "            down_tensor = down_tensor + lora_tensor\n",
    "        return down_tensor\n",
    "\n",
    "\n",
    "class MOE(nn.Module):\n",
    "    def __init__(self, hidden_size: int, device: str, num_experts: int = 8, expansion_factor: int = 4, dropout_ratio: float = 0.1, lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "        self.device = device\n",
    "        self.experts = nn.ModuleList([FeedForward(hidden_size, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Flatten for better manipulation, this is ok because tokens are independent at this stage\n",
    "        batch_size, seq_len, hidden_size = tensor.shape\n",
    "        flat_tensor = tensor.reshape(batch_size * seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the gating network and select experts\n",
    "        tensor = self.gate(flat_tensor)\n",
    "        tensor = F.softmax(tensor, dim=-1)\n",
    "\n",
    "        # The output of this step is a tensor of shape [batch_size * seq_len, 2] with element i in the second dimension representing ith expert selected for this token\n",
    "        value_tensor, index_tensor = tensor.topk(k=2, dim=-1)\n",
    "\n",
    "        # Find the load balancing loss\n",
    "        counts = torch.bincount(index_tensor[:, 0], minlength=self.num_experts)\n",
    "        frequencies = counts.float() / (batch_size * seq_len) # This is the hard one-hot frequency\n",
    "        probability = tensor.mean(0) # This is the soft probability\n",
    "        load_balancing_loss = (probability * frequencies).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # Normalize top1 and top2 score\n",
    "        top_expert_score = value_tensor[:, 0]\n",
    "        second_expert_score = value_tensor[:, 1]\n",
    "        total_score = top_expert_score + second_expert_score\n",
    "        top_expert_score = top_expert_score / total_score\n",
    "        second_expert_score = second_expert_score / total_score\n",
    "\n",
    "        # Split into top 2 experts\n",
    "        split_tensors = torch.split(index_tensor, 1, dim=-1)\n",
    "        top_expert, second_expert = split_tensors[0], split_tensors[1]\n",
    "        indices = torch.arange(batch_size * seq_len).unsqueeze(-1).to(self.device)\n",
    "        top_expert = torch.cat((indices, top_expert), dim=-1)\n",
    "        second_expert = torch.cat((indices, second_expert), dim=-1)\n",
    "\n",
    "        # Sort based on expert selection\n",
    "        top_expert = top_expert[top_expert[:,1].argsort()]\n",
    "        second_expert = second_expert[second_expert[:,1].argsort()]\n",
    "\n",
    "        # Count how many tokens goes to each expert\n",
    "        top_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            top_expert_counts[i] = (top_expert[:,1] == i).sum()\n",
    "        top_expert_counts = top_expert_counts.tolist()\n",
    "\n",
    "        second_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            second_expert_counts[i] = (second_expert[:,1] == i).sum()\n",
    "        second_expert_counts = second_expert_counts.tolist()\n",
    "\n",
    "        # Split input tokens for each expert\n",
    "        top_expert_tokens = flat_tensor[top_expert[:,0]]\n",
    "        second_expert_tokens = flat_tensor[second_expert[:,0]]\n",
    "\n",
    "        # Split into a list of tensors, element i tensor is for ith expert.\n",
    "        top_expert_tokens = torch.split(top_expert_tokens, top_expert_counts, dim=0)\n",
    "        second_expert_tokens = torch.split(second_expert_tokens, second_expert_counts, dim=0)\n",
    "\n",
    "        # Input into each expert and obtain results in a list\n",
    "        top_expert_outputs = [self.experts[i](top_expert_tokens[i], fine_tuning) if top_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(self.device) for i in range(self.num_experts)]\n",
    "        second_expert_outputs = [self.experts[i](second_expert_tokens[i], fine_tuning) if second_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(self.device) for i in range(self.num_experts)]\n",
    "\n",
    "        # Combine outputs\n",
    "        top_expert_outputs = torch.cat(top_expert_outputs, dim=0)\n",
    "        second_expert_outputs = torch.cat(second_expert_outputs, dim=0)\n",
    "\n",
    "        # Re-index the output back to original token order\n",
    "        flat_top_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float16).to(self.device)\n",
    "        flat_top_expert_tensor.index_copy_(0, top_expert[:, 0], top_expert_outputs)\n",
    "\n",
    "        flat_second_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float16).to(self.device)\n",
    "        flat_second_expert_tensor.index_copy_(0, second_expert[:, 0], second_expert_outputs)\n",
    "\n",
    "        # Find final output tensor based on weight between top and second expert\n",
    "        final_tensor = top_expert_score.unsqueeze(-1) * flat_top_expert_tensor + second_expert_score.unsqueeze(-1) * flat_second_expert_tensor\n",
    "\n",
    "        # Reshape to original [batch_size, seq_len, hidden_size]\n",
    "        final_tensor = final_tensor.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return final_tensor, load_balancing_loss\n",
    "\n",
    "\n",
    "class VisionLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 device: str,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts: int = 8,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.use_moe = use_moe\n",
    "        self.device = device\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attention = VisionAttention(hidden_dim, head_dim, q_head, kv_head, lora_rank=lora_rank)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        if self.use_moe:\n",
    "            self.moe = MOE(hidden_dim, device, num_experts=num_experts, expansion_factor=expansion_factor,\n",
    "                           dropout_ratio=dropout_ratio, lora_rank=lora_rank)\n",
    "        else:\n",
    "            self.ffn = FeedForward(hidden_dim, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio,\n",
    "                                  lora_rank=lora_rank)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False):\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = self.attention(tensor, attention_mask=attention_mask, fine_tuning=fine_tuning)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm2(tensor)\n",
    "        if self.use_moe:\n",
    "            tensor, load_balancing_loss = self.moe(tensor, fine_tuning=fine_tuning)\n",
    "        else:\n",
    "            tensor = self.ffn(tensor, fine_tuning=fine_tuning)\n",
    "            load_balancing_loss = torch.tensor(0.0, dtype=tensor.dtype, device=self.device)\n",
    "\n",
    "        tensor += skip_connection\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size: tuple,\n",
    "                 num_classes: int = 1,\n",
    "                 patch_size: int = 8,\n",
    "                 in_channels: int = 3,\n",
    "                 num_layer: int = 3,\n",
    "                 hidden_dim: int = 1024,\n",
    "                 expansion_factor: int = 8,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = 16,\n",
    "                 kv_head: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = True,\n",
    "                 num_experts: int = 8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_layer = num_layer\n",
    "        self.load_balancing_loss_weight = load_balancing_loss_weight\n",
    "        self.fine_tuning = fine_tuning\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "\n",
    "        # Calculate number of patches (sequence length)\n",
    "        self.num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size) + 1  # +1 for cls token\n",
    "\n",
    "        if q_head == None:\n",
    "            q_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if kv_head == None:\n",
    "            kv_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if hidden_dim % (head_dim * q_head) != 0 or hidden_dim % (head_dim * kv_head):\n",
    "            raise ValueError(\"Error: hidden_dim or projection_dim (if specified) must be divisible by the product of the number of q or kv heads and the head dimension.\")\n",
    "\n",
    "        # Create transformer layers\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.transformer.append(VisionLayer(\n",
    "                hidden_dim, head_dim, q_head, kv_head, device,\n",
    "                expansion_factor=expansion_factor,\n",
    "                dropout_ratio=dropout_ratio,\n",
    "                use_moe=use_moe,\n",
    "                num_experts=num_experts,\n",
    "                lora_rank=lora_rank\n",
    "            ))\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Final classifier head\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def begin_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = True\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora\" not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def exit_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"positions\" in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Handle input shape\n",
    "        if len(x.shape) == 3:  # [batch_size, 64, 72]\n",
    "            # Reshape to [batch_size, channels, height, width]\n",
    "            # Assuming the input is grayscale (1 channel)\n",
    "            batch_size, height, width = x.shape\n",
    "            x = x.unsqueeze(1)  # Add channel dimension [batch_size, 1, 64, 72]\n",
    "\n",
    "        # Apply patch embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # Track load-balancing across layers (only if MoE is used)\n",
    "        load_balancing_sum = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.transformer:\n",
    "            x, load_balancing_loss = layer(x, fine_tuning=self.fine_tuning)\n",
    "            load_balancing_sum += load_balancing_loss\n",
    "\n",
    "        load_balancing_loss = (load_balancing_sum / self.num_layer) * self.load_balancing_loss_weight\n",
    "\n",
    "        # Apply output normalization\n",
    "        x = self.output_norm(x)\n",
    "\n",
    "        # Use CLS token for classification\n",
    "        x = x[:, 0]  # Take only the CLS token\n",
    "\n",
    "        # Apply classifier\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x, load_balancing_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D5ghkfD4V2Z3"
   },
   "outputs": [],
   "source": [
    "data0 = np.load('Run357479_Dataset_iodic.npy')\n",
    "data1 = np.load('Run355456_Dataset_jqkne.npy')\n",
    "\n",
    "# Create labels: 0 for class 0 and 1 for class 1\n",
    "labels_0 = np.zeros((data0.shape[0],), dtype=np.int32)\n",
    "labels_1 = np.ones((data1.shape[0],), dtype=np.int32)\n",
    "\n",
    "# Concatenate data and labels\n",
    "data = np.concatenate([data0, data1], axis=0)  # Shape (20000, 64, 72)\n",
    "labels = np.concatenate([labels_0, labels_1], axis=0)  # Shape (20000,)\n",
    "\n",
    "# Shuffle data and labels together\n",
    "indices = np.random.permutation(data.shape[0])\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Seperate into training, validation and testing set.\n",
    "n_total = data.shape[0]\n",
    "n_train = int(n_total * 0.8)\n",
    "n_val = int(n_total * 0.1)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "train_data, val_data, test_data = data[:n_train], data[n_train:n_train + n_val], data[n_train + n_val:]\n",
    "train_labels, val_labels, test_labels = labels[:n_train], labels[n_train:n_train + n_val], labels[n_train + n_val:]\n",
    "\n",
    "# Compute mean and std from the training data only\n",
    "mean = train_data.mean()\n",
    "std = train_data.std()\n",
    "\n",
    "# Apply the same transformation to train, val, test\n",
    "train_data = (train_data - mean) / (std + 1e-7)\n",
    "val_data = (val_data - mean) / (std + 1e-7)\n",
    "test_data = (test_data - mean) / (std + 1e-7)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_data_tensor = torch.tensor(val_data, dtype=torch.float32)\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor.float().unsqueeze(1))\n",
    "val_dataset = TensorDataset(val_data_tensor, val_labels_tensor.float().unsqueeze(1))\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor.float().unsqueeze(1))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint loading\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    validation_loss = checkpoint.get('validation_loss', float('inf'))\n",
    "    print(f\"Loaded checkpoint from epoch {epoch} with validation loss {validation_loss:.6f}\")\n",
    "    return epoch\n",
    "\n",
    "# Checkpoint saving\n",
    "def save_checkpoint(model, optimizer, epoch, validation_loss):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'validation_loss': validation_loss\n",
    "    }\n",
    "    torch.save(checkpoint, f\"{save_dir}/vit_checkpoint_epoch_{epoch}.pt\")\n",
    "    # Save best model separately\n",
    "    if epoch == 0 or validation_loss < min(loss_valid):\n",
    "        torch.save(checkpoint, f\"{save_dir}/vit_best_model.pt\")\n",
    "        print(f\"Saved best model with validation loss: {validation_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 10774253 parameters.\n",
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "vit = VisionTransformer(\n",
    "    image_size=(64, 72),    # Your input image dimensions\n",
    "    patch_size=8,           # Size of each patch\n",
    "    in_channels=1,          # We technically don't have channel value here.\n",
    "    num_classes=1,          # Number of output classes\n",
    "    num_layer=3,            # Number of transformer layers\n",
    "    hidden_dim=256,        # Hidden dimension\n",
    "    expansion_factor=4,     # Expansion factor for FFN\n",
    "    head_dim=64,            # Dimension of each attention head\n",
    "    q_head=4,              # Number of query heads\n",
    "    kv_head=1,              # Number of key/value heads\n",
    "    use_moe=True,           # Whether to use Mixture of Experts\n",
    "    num_experts=4\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint if available\n",
    "current_epoch = 0\n",
    "if checkpoint_filepath is not None and checkpoint_filepath != \"\":\n",
    "    current_epoch = load_checkpoint(vit, optimizer, checkpoint_filepath) + 1\n",
    "\n",
    "print(f\"This model has {sum(p.numel() for p in vit.parameters())} parameters.\")\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification loss since num_classes=1\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader), eta_min=1e-5)\n",
    "scaler = amp.GradScaler() # Initialize gradient scaler for mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss tracking lists\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "accuracy_train = []\n",
    "accuracy_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 16/16 [00:06<00:00,  2.63it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Loss: 0.579171, Accuracy: 74.58%\n",
      "Validation - Loss: 0.027781, Accuracy: 99.70%\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 16/16 [00:04<00:00,  3.26it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Loss: 0.016199, Accuracy: 99.88%\n",
      "Validation - Loss: 0.010464, Accuracy: 100.00%\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 16/16 [00:04<00:00,  3.29it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Loss: 0.010416, Accuracy: 100.00%\n",
      "Validation - Loss: 0.010539, Accuracy: 100.00%\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 16/16 [00:04<00:00,  3.20it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Loss: 0.010518, Accuracy: 100.00%\n",
      "Validation - Loss: 0.010374, Accuracy: 100.00%\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 16/16 [00:05<00:00,  3.15it/s]\n",
      "Validation: 100%|██████████| 2/2 [00:00<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Loss: 0.010164, Accuracy: 100.00%\n",
      "Validation - Loss: 0.010069, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(current_epoch, epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    vit.train()\n",
    "    loss_train_epoch = []\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).float()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with amp.autocast():\n",
    "            outputs, load_balancing_loss = vit(inputs)\n",
    "            loss = criterion(outputs, targets) + load_balancing_loss\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(vit.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step with scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += targets.size(0)\n",
    "        correct_train += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = np.mean(loss_train_epoch)\n",
    "    epoch_accuracy = 100 * correct_train / total_train\n",
    "    loss_train.append(epoch_loss)\n",
    "    accuracy_train.append(epoch_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    vit.eval()\n",
    "    loss_val_epoch = []\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            with amp.autocast():\n",
    "                outputs, load_balancing_loss = vit(inputs)\n",
    "                loss = criterion(outputs, targets) + load_balancing_loss\n",
    "            \n",
    "            # Record loss\n",
    "            loss_val_epoch.append(loss.item())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Calculate epoch validation statistics\n",
    "    epoch_val_loss = np.mean(loss_val_epoch)\n",
    "    epoch_val_accuracy = 100 * correct_val / total_val\n",
    "    loss_valid.append(epoch_val_loss)\n",
    "    accuracy_valid.append(epoch_val_accuracy)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Training - Loss: {epoch_loss:.6f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    print(f\"Validation - Loss: {epoch_val_loss:.6f}, Accuracy: {epoch_val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Plot and save training progress\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_train, label=\"Training loss\")\n",
    "    plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracy_train, label=\"Training accuracy\")\n",
    "    plt.plot(accuracy_valid, label=\"Validation accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/training_progress_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Save checkpoint\n",
    "save_checkpoint(vit, optimizer, epoch, epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - Loss: 0.010067, Accuracy: 100.00%\n",
      "AUC: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Lists to store true labels and predicted probabilities\n",
    "all_targets = []\n",
    "all_probs = []\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "vit.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).float()\n",
    "\n",
    "        with amp.autocast():\n",
    "            outputs, load_balancing_loss = vit(inputs)\n",
    "            loss = criterion(outputs, targets) + load_balancing_loss\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        # Compute probabilities using sigmoid\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        # Calculate binary predictions for accuracy\n",
    "        predicted = (probabilities > 0.5).float()\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Append to lists (move to CPU and convert to numpy)\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        all_probs.append(probabilities.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"Test set - Loss: {avg_test_loss:.6f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Concatenate all the batches\n",
    "all_targets = np.concatenate(all_targets)\n",
    "all_probs = np.concatenate(all_probs)\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(all_targets, all_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC: {:.4f}\".format(roc_auc))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', color='gray')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': vit.state_dict(),\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_loss': avg_test_loss\n",
    "}, f\"{save_dir}/vit_final_model.pt\")\n",
    "\n",
    "print(\"Test completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for visualization of training data, not used in actual training but kept here for your reference\n",
    "non_zero_values = data1[data1 != 0]\n",
    "\n",
    "# Calculate statistics\n",
    "min_non_zero = np.min(non_zero_values)\n",
    "max_non_zero = np.max(non_zero_values)\n",
    "mean_non_zero = np.mean(non_zero_values)\n",
    "median_non_zero = np.median(non_zero_values)\n",
    "std_non_zero = np.std(non_zero_values)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Histogram with KDE\n",
    "sns.histplot(non_zero_values, kde=True, ax=ax1)\n",
    "ax1.set_title('Distribution of Non-Zero Values', fontsize=14)\n",
    "ax1.set_xlabel('Value', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.axvline(min_non_zero, color='r', linestyle='--', label=f'Min: {min_non_zero:.2f}')\n",
    "ax1.axvline(max_non_zero, color='g', linestyle='--', label=f'Max: {max_non_zero:.2f}')\n",
    "ax1.axvline(mean_non_zero, color='b', linestyle='-', label=f'Mean: {mean_non_zero:.2f}')\n",
    "ax1.axvline(median_non_zero, color='purple', linestyle='-.', label=f'Median: {median_non_zero:.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(x=non_zero_values, ax=ax2)\n",
    "ax2.set_title('Boxplot of Non-Zero Values', fontsize=14)\n",
    "ax2.set_xlabel('Value', fontsize=12)\n",
    "\n",
    "# Add text with statistics\n",
    "stats_text = (f\"Non-zero count: {len(non_zero_values)}\\n\"\n",
    "              f\"Min: {min_non_zero:.2f}\\n\"\n",
    "              f\"Max: {max_non_zero:.2f}\\n\"\n",
    "              f\"Mean: {mean_non_zero:.2f}\\n\"\n",
    "              f\"Median: {median_non_zero:.2f}\\n\"\n",
    "              f\"Std Dev: {std_non_zero:.2f}\")\n",
    "\n",
    "fig.text(0.15, 0.01, stats_text, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig('non_zero_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Print summary to console\n",
    "print(f\"Non-zero value summary:\")\n",
    "print(f\"Count: {len(non_zero_values)}\")\n",
    "print(f\"Min: {min_non_zero:.2f}\")\n",
    "print(f\"Max: {max_non_zero:.2f}\")\n",
    "print(f\"Mean: {mean_non_zero:.2f}\")\n",
    "print(f\"Median: {median_non_zero:.2f}\")\n",
    "print(f\"Standard deviation: {np.std(non_zero_values):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
